

# import basic libraries
import os
import os.path as p
from subprocess import call, PIPE
import yaml
import pandas as pd
# clean code
from typing import Union, Tuple
from os import PathLike
from pandas.core.groupby.generic import DataFrameGroupBy
## WellsWood Lab libraries
import ampal
import isambard.modelling as modelling
from pdbUtils import pdbUtils
##########################################################################
def directed_fpocket(protName: str,
                    runDir: Union[PathLike, str],
                    pdbFile: Union[PathLike, str],
                    targetPocketResidues: dict) -> Tuple[list, dict]:
    """
    Uses the command-line program FPocket to detect binding pocket
    The user-specified dictonary "targetPocketResidues" is used to find the correct pocket
        by selecting the pocket with that contains the most user-specified residues
        this should make this method insensitive to FPocket being inconistant with user-specified residues
    Also generates the docking box central coordinates as an average position of the identified binding pocket
    """

    ## change to simulation directory (fpocket gets confused otherwise!)
    os.chdir(runDir)
    ## set some FPocket parameters
    minSphereSize: str = "3.0"
    maxSphereSize: str = "6.0"
    ## run FPocket
    call(["fpocket", "-f", pdbFile, "-m", minSphereSize,
         "-M", maxSphereSize], stdout=PIPE)
    ## look at all the pockets that FPocket finds, count how many target residues are in each pocket  
    fpocketOutDir: Union[PathLike, str] = p.join(runDir, f"{protName}_out", "pockets")
    targetResidueCounts: dict = {}
    ## loop through all pockets generated by fpocket, skip non-pdb output files
    for file in os.listdir(fpocketOutDir):
        ## init a counter 
        targetResidueCount = 0
        fileData = p.splitext(file)
        if not fileData[1] == ".pdb":
            continue
        pocketPdb = p.join(fpocketOutDir, file)
        ## load as a pdb dataframe
        pocketDf = pdbUtils.pdb2df(pocketPdb)
        ## check for targetResidues in pdb dataframe and add to count
        for targetRes in targetPocketResidues:
            targetDf: pd.DataFrame = pocketDf[(pocketDf["CHAIN_ID"] == targetRes["CHAIN_ID"])
                                    & (pocketDf["RES_NAME"] == targetRes["RES_NAME"])
                                    & (pocketDf["RES_ID"] == targetRes["RES_ID"])]
            if len(targetDf) > 0:
                targetResidueCount += 1
        ## add counter to a dictionary
        pocketId = fileData[0].split("_")[0]
        targetResidueCounts.update({pocketId: targetResidueCount})

    # select the pocket with the most active site residues
    bindingPocketId = max(targetResidueCounts, key=targetResidueCounts.get)
    bindingPocketPdb = p.join(fpocketOutDir, f"{bindingPocketId}_atm.pdb")
    bindingPocketDf = pdbUtils.pdb2df(bindingPocketPdb)
    ## calculate the center of teh docking box as the average coords of teh binding pocket
    boxCenter = [
        bindingPocketDf["X"].mean(),
        bindingPocketDf["Y"].mean(),
        bindingPocketDf["Z"].mean()]
    ## create a dictionary containing chainId, resName and resId of pocket residues
    ## write to yaml file for further steps in pipelines
    pocketChains = bindingPocketDf["CHAIN_ID"].to_list()
    pocketResNames = bindingPocketDf["RES_NAME"].to_list()
    pocketResIds = bindingPocketDf["RES_ID"].to_list()

    uniqueResidues = set(zip(pocketChains, pocketResNames, pocketResIds))
    pocketResidues = [{"CHAIN_ID": chainId,
                       "RES_NAME": resName,
                       "RES_ID": resId} for chainId,
                      resName,
                      resId in uniqueResidues]
    pocketResidues = sorted(
        pocketResidues, key=lambda x: (
            x['CHAIN_ID'], x['RES_ID']))

    # dump to yaml
    pocketResiduesYaml = p.join(runDir, f"{protName}_pocket_residues.yaml")
    with open(pocketResiduesYaml, "w") as yamlFile:
        yaml.dump(pocketResidues, yamlFile, default_flow_style=False)


    return boxCenter, pocketResidues
##########################################################################

def pocket_residues_to_alainine(
        protName: str,
        pdbFile: Union[os.PathLike, str],
        residuesToAlanine: dict,
        dockingOrder: dict ,
        outDir: Union[os.PathLike, str]) -> Union[os.PathLike, str]:
    """
    This is the "void" portion of voidDock
    This function uses Scwrl4 to mutate all pocketResidues (identified by fpocket) to alanine
    Residues specified in "keepResidues" entry in config.yaml will not be mutated
    """
    if "keepResidues" in dockingOrder:
        keepResidues: dict = dockingOrder["keepResidues"]
    else:
        keepResidues = []
    # remove residues in keepResidues, these will not be changed to Alanine
    residuesToAlanine: list = [residue for residue in residuesToAlanine if residue not in keepResidues]
    protDf: pd.DataFrame = pdbUtils.pdb2df(pdbFile)
    uniquePocketResidues: set = set((d['CHAIN_ID'], d['RES_ID'])
                             for d in residuesToAlanine)
    


    ## change residue names to new sequence
    def change_res_name(row: pd.Series) -> str:
        """
        Small sub-function to change the three-letter amino acid code in a pdb dataframe to alanine
        if it matches with specified residues
        """
        if (row['CHAIN_ID'], row['RES_ID']) in uniquePocketResidues:
            return 'ALA'
        else:
            return row['RES_NAME']
        
    protDf['RES_NAME'] = protDf.apply(change_res_name, axis=1)


    ## get dict of one-letter sequences 
    newSequences: dict  = pdbDf2seq(protDf)

    """
    Use scwrl4 to mutate each chain of the pdb file separately
    then merge them together with pdbUtils
    """
    ## init empty dict to store pdb files
    tmpChainPdbs = {}
    ## loop through chains
    for chainId in protDf['CHAIN_ID'].unique():
        chainDf = protDf[protDf['CHAIN_ID'] == chainId]
        ## write chainDf to temporary pdb file
        tmpChainPdb = p.join(outDir, f"{protName}_{chainId}.pdb")
        pdbUtils.df2pdb(chainDf, tmpChainPdb)
        ## load chain pdb as an ampal object
        chainAmpal = ampal.load_pdb(tmpChainPdb)
        ## get sequence for this chain
        newChainSequence = newSequences[chainId]
        ## mutate and repack using scwrl4, overwrite temporary pdb file
        packedChain = modelling.pack_side_chains_scwrl(chainAmpal, [newChainSequence])
        finalPdbString = packedChain.make_pdb(ligands=False)
        with open(tmpChainPdb, "w") as file:
            file.write(finalPdbString)
        tmpChainPdbs[chainId] = tmpChainPdb
    ## merge 
    tmpChainPdbs: list = [tmpChainPdbs[chainId] for chainId in tmpChainPdbs]
    alaPdb = p.join(outDir, f"{protName}_pocketAla.pdb")
    pdbUtils.mergePdbs(tmpChainPdbs, alaPdb)
    ## remove temporary pdb files
    for pdbFile in tmpChainPdbs:
        os.remove(pdbFile)

    return alaPdb
##########################################################################
def pdbDf2seq(df: pd.DataFrame) -> dict:
    """
    Simple script that generates a sequence string from a pdb dataframe
    Returns:
    - sequences: a dict with {ChainID: Sequence}
    """
    ## init dict
    aminoAcidsThreeToOne: dict= {
    'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F', 
    'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L',
    'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R',
    'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'
}
    ##
    df['oneLetter']= df['RES_NAME'].map(aminoAcidsThreeToOne)
    uniqueResidues: pd.DataFrame = df[['CHAIN_ID', 'RES_ID', 'oneLetter']].drop_duplicates(subset=['CHAIN_ID', 'RES_ID'])

    chainGroups: DataFrameGroupBy = uniqueResidues.sort_values(by=['CHAIN_ID', 'RES_ID']).groupby('CHAIN_ID')
    sequences = {}
    for chain, group in chainGroups:
        sequences[chain] = ''.join(group['oneLetter'])
    # Print all sequences, each represented by a different chain

    return sequences
##########################################################################
